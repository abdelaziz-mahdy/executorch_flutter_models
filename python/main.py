#!/usr/bin/env python3
"""
ExecuTorch Flutter - Main Script

Unified command-line tool for model export and validation.

Usage:
    python main.py                          # Export models (default)
    python main.py export                   # Export models
    python main.py export --mobilenet       # Export MobileNet only
    python main.py export --yolo yolo11n    # Export YOLO11n
    python main.py validate                 # Validate all models
"""

import sys
import argparse
from pathlib import Path

# Export functions
from validate_all_models import ModelValidator

# Validation functions
import torch
import torchvision.models as models


def export_mobilenet(output_dir="..", backends=None):
    """Export MobileNet V3 Small with multiple backend support."""
    print("\n" + "="*70)
    print("  Exporting MobileNet V3 Small")
    print("="*70 + "\n")

    try:
        import torch
        from pathlib import Path
        from executorch_exporter import ExecuTorchExporter, ExportConfig

        # Default backends: xnnpack for all platforms, coreml/mps for Apple, vulkan for Android
        if backends is None:
            backends = ['xnnpack', 'coreml', 'mps', 'vulkan']

        # Load model
        model = models.mobilenet_v3_small(weights='DEFAULT').eval()
        sample_inputs = (torch.randn(1, 3, 224, 224),)

        # Create exporter
        exporter = ExecuTorchExporter()

        # Filter backends to only available ones
        available_backends = [b for b in backends if exporter.available_backends.get(b, False)]

        if not available_backends:
            print(f"âš ï¸  No available backends from requested: {backends}")
            print(f"   Available backends: {[k for k, v in exporter.available_backends.items() if v]}")
            return False

        print(f"ğŸ“¦ Exporting to backends: {available_backends}")

        # Output to mobilenet subdirectory
        mobilenet_output_dir = str(Path(output_dir) / "mobilenet")

        # Create export config
        config = ExportConfig(
            model_name='mobilenet_v3_small',
            backends=available_backends,
            output_dir=mobilenet_output_dir,
            quantize=False,
            input_shapes=[[1, 3, 224, 224]],
            input_dtypes=['float32']
        )

        # Export to all backends
        results = exporter.export_model(model, sample_inputs, config)

        # Check success
        successful = [r for r in results if r.success]
        failed = [r for r in results if not r.success]

        if successful:
            print(f"\nâœ… Successfully exported {len(successful)}/{len(results)} backends")
            for result in successful:
                print(f"   â€¢ {result.backend}: {result.output_path.split('/')[-1]} ({result.file_size_mb:.1f} MB)")

        if failed:
            print(f"\nâš ï¸  Failed {len(failed)} backend(s):")
            for result in failed:
                print(f"   â€¢ {result.backend}: {result.error_message}")

        return len(successful) > 0

    except Exception as e:
        print(f"âŒ Export failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_yolo(model_name="yolo11n", output_dir="..", backends=None):
    """Export YOLO model with multiple backend support."""
    print("\n" + "="*70)
    print(f"  Exporting {model_name.upper()}")
    print("="*70 + "\n")

    try:
        import torch
        import numpy as np
        from pathlib import Path
        from ultralytics import YOLO
        from executorch_exporter import ExecuTorchExporter, ExportConfig

        # Default backends: xnnpack for all platforms, coreml/mps for Apple, vulkan for Android
        if backends is None:
            backends = ['xnnpack', 'coreml', 'mps', 'vulkan']

        # Load YOLO model
        model = YOLO(f"{model_name}.pt")

        # Run a dummy prediction to initialize the model
        np_dummy_tensor = np.ones((640, 640, 3))
        model.predict(np_dummy_tensor, imgsz=(640, 640), device="cpu")

        # Get the PyTorch model and put in eval mode
        pt_model = model.model.cpu().eval()

        # Prepare sample inputs (640x640 for YOLO)
        sample_inputs = (torch.randn(1, 3, 640, 640),)

        # Create exporter
        exporter = ExecuTorchExporter()

        # Filter backends to only available ones
        available_backends = [b for b in backends if exporter.available_backends.get(b, False)]

        if not available_backends:
            print(f"âš ï¸  No available backends from requested: {backends}")
            print(f"   Available backends: {[k for k, v in exporter.available_backends.items() if v]}")
            return False

        print(f"ğŸ“¦ Exporting to backends: {available_backends}")

        # Output to yolo subdirectory
        yolo_output_dir = str(Path(output_dir) / "yolo")

        # Create export config
        config = ExportConfig(
            model_name=model_name,
            backends=available_backends,
            output_dir=yolo_output_dir,
            quantize=False,
            input_shapes=[[1, 3, 640, 640]],
            input_dtypes=['float32']
        )

        # Export to all backends
        results = exporter.export_model(pt_model, sample_inputs, config)

        # Check success
        successful = [r for r in results if r.success]
        failed = [r for r in results if not r.success]

        if successful:
            print(f"\nâœ… Successfully exported {len(successful)}/{len(results)} backends")
            for result in successful:
                print(f"   â€¢ {result.backend}: {result.output_path.split('/')[-1]} ({result.file_size_mb:.1f} MB)")

        if failed:
            print(f"\nâš ï¸  Failed {len(failed)} backend(s):")
            for result in failed:
                print(f"   â€¢ {result.backend}: {result.error_message}")

        # Clean up downloaded model files (handles both .pt and variant names like yolov5nu.pt)
        for pt_file in Path.cwd().glob("*.pt"):
            if pt_file.stem.startswith(model_name):
                pt_file.unlink()
                print(f"   Cleaned up: {pt_file.name}")

        return len(successful) > 0

    except Exception as e:
        print(f"âŒ Export failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_gemma(model_name="gemma-3-270m", output_dir=".."):
    """Export Gemma text generation model using Optimum ExecuTorch.

    Supported models:
    - gemma-3-270m: 240 MB (text-only, 270M parameters)
    - gemma-3-1b: 892 MB - 1.5 GB (text-only, 1B parameters)

    IMPORTANT: Gemma models are gated on HuggingFace and require:
    1. Request access at https://huggingface.co/google/gemma-3-270m-it
    2. Authenticate with: hf auth login
    3. Install optimum-executorch from source:
       git clone https://github.com/huggingface/optimum-executorch.git
       cd optimum-executorch && pip install '.[dev]' && python install_dev.py
    """
    print("\n" + "="*70)
    print(f"  Exporting {model_name.upper()}")
    print("="*70 + "\n")

    try:
        import subprocess
        from pathlib import Path
        import json
        import shutil

        # Map model name to HuggingFace model ID
        model_id = f"google/{model_name}-it"

        print(f"ğŸ“¦ Exporting {model_name} using Optimum ExecuTorch...")
        print(f"   Model ID: {model_id}")
        print(f"   This may take several minutes on first run...")

        # Create temporary output directory (use absolute path for optimum-cli)
        temp_output = Path(output_dir).resolve() / f"{model_name}_temp"
        temp_output.mkdir(parents=True, exist_ok=True)

        # Use optimum-cli to export the model
        # Note: Removed --use_custom_sdpa and --use_custom_kv_cache flags
        # as they require ExecuTorch features not available in v0.7.0
        cmd = [
            "optimum-cli", "export", "executorch",
            "--model", model_id,
            "--task", "text-generation",
            "--recipe", "xnnpack",
            "--qlinear", "8da4w",
            "--qembedding", "8w",
            "--output_dir", str(temp_output)
        ]

        print(f"\nğŸ”„ Running optimum-cli export...")
        print(f"   Command: {' '.join(cmd)}")

        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            print(f"âŒ Export failed with exit code {result.returncode}")
            print(f"   stderr: {result.stderr}")
            return False

        print(f"âœ… Model exported successfully!")

        # Find and rename the .pte file
        pte_files = list(temp_output.glob("*.pte"))
        if not pte_files:
            print(f"âŒ No .pte file found in {temp_output}")
            return False

        pte_file = pte_files[0]
        # Output to gemma subdirectory
        gemma_output_dir = Path(output_dir).resolve() / "gemma"
        gemma_output_dir.mkdir(parents=True, exist_ok=True)
        final_pte = gemma_output_dir / f"{model_name}_xnnpack.pte"

        shutil.move(str(pte_file), str(final_pte))
        file_size_mb = final_pte.stat().st_size / (1024 * 1024)
        print(f"âœ… Model file: {final_pte.name} ({file_size_mb:.1f} MB)")

        # Load tokenizer and save vocabulary
        from transformers import AutoTokenizer

        tokenizer = AutoTokenizer.from_pretrained(model_id)

        # Save simplified vocabulary for Dart
        vocab = tokenizer.get_vocab()
        vocab_file = gemma_output_dir / f"{model_name}_vocab.json"
        with open(vocab_file, "w") as f:
            json.dump(vocab, f, indent=2)

        print(f"âœ… Vocabulary saved: {vocab_file.name} ({len(vocab)} tokens)")

        # Save tokenizer config
        tokenizer_config = {
            "vocab_size": tokenizer.vocab_size,
            "bos_token": tokenizer.bos_token,
            "eos_token": tokenizer.eos_token,
            "pad_token": tokenizer.pad_token if tokenizer.pad_token else tokenizer.eos_token,
            "model_max_length": 2048,
            "special_tokens": {
                "bos_token_id": tokenizer.bos_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "pad_token_id": tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id,
            }
        }

        tokenizer_config_file = gemma_output_dir / f"{model_name}_tokenizer_config.json"
        with open(tokenizer_config_file, "w") as f:
            json.dump(tokenizer_config, f, indent=2)

        print(f"âœ… Tokenizer config saved: {tokenizer_config_file.name}")

        # Clean up temp directory
        shutil.rmtree(temp_output)

        return True

    except ImportError as e:
        print(f"âŒ Import error: {e}")
        print(f"\nğŸ’¡ Please install optimum-executorch from source:")
        print(f"   git clone https://github.com/huggingface/optimum-executorch.git")
        print(f"   cd optimum-executorch && pip install '.[dev]' && python install_dev.py")
        return False
    except Exception as e:
        print(f"âŒ Export failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_labels(output_dir=".."):
    """Export COCO and ImageNet labels to model directories."""
    print("\n" + "="*70)
    print("  Generating Label Files")
    print("="*70 + "\n")

    # COCO labels (for YOLO)
    coco_labels = [
        "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",
        "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat",
        "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack",
        "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball",
        "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket",
        "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
        "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair",
        "couch", "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse",
        "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator",
        "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
    ]

    output_path = Path(output_dir)

    # Save COCO labels to yolo directory
    yolo_dir = output_path / "yolo"
    yolo_dir.mkdir(parents=True, exist_ok=True)
    coco_file = yolo_dir / "labels.txt"
    coco_file.write_text('\n'.join(coco_labels))
    print(f"âœ… COCO labels: {coco_file}")
    print(f"   ({len(coco_labels)} classes)")

    # Note: ImageNet labels are large (1000 classes), typically downloaded from torchvision
    # For now, we just note where they should go
    mobilenet_dir = output_path / "mobilenet"
    mobilenet_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ MobileNet labels should be at: {mobilenet_dir / 'labels.txt'}")


def generate_index_json(output_dir=".."):
    """Generate index.json with metadata for all models.

    The index.json contains:
    - name: model filename
    - hash: SHA256 hash for cache invalidation
    - size: file size in bytes
    - category: model category (mobilenet, yolo, gemma)
    - backend: backend type (xnnpack, coreml, mps, vulkan)
    - inputSize: model input size
    - description: human-readable description
    """
    import json
    import hashlib

    print("\n" + "="*70)
    print("  Generating index.json")
    print("="*70 + "\n")

    output_path = Path(output_dir)

    # Model categories and their configurations
    model_configs = {
        "mobilenet": {
            "inputSize": 224,
            "labelsFile": "labels.txt",
            "labelsUrl": "https://raw.githubusercontent.com/abdelaziz-mahdy/executorch_flutter_models/main/mobilenet/labels.txt",
        },
        "yolo": {
            "inputSize": 640,
            "labelsFile": "labels.txt",
            "labelsUrl": "https://raw.githubusercontent.com/abdelaziz-mahdy/executorch_flutter_models/main/yolo/labels.txt",
        },
        "gemma": {
            "inputSize": None,
            "labelsFile": None,
            "labelsUrl": None,
        },
    }

    # Backend descriptions
    backend_info = {
        "xnnpack": {"platforms": ["android", "ios", "macos", "web"], "description": "CPU-optimized (XNNPACK)"},
        "coreml": {"platforms": ["ios", "macos"], "description": "Apple Neural Engine (CoreML)"},
        "mps": {"platforms": ["ios", "macos"], "description": "Apple GPU (Metal Performance Shaders)"},
        "vulkan": {"platforms": ["android", "linux"], "description": "GPU-accelerated (Vulkan)"},
    }

    models = []

    # Scan each category directory
    for category, config in model_configs.items():
        category_dir = output_path / category
        if not category_dir.exists():
            continue

        # Find all .pte files
        for pte_file in sorted(category_dir.glob("*.pte")):
            # Calculate SHA256 hash
            sha256_hash = hashlib.sha256()
            with open(pte_file, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    sha256_hash.update(chunk)

            file_hash = sha256_hash.hexdigest()
            file_size = pte_file.stat().st_size
            file_name = pte_file.name

            # Extract backend from filename (e.g., "mobilenet_v3_small_xnnpack.pte" -> "xnnpack")
            backend = file_name.replace(".pte", "").split("_")[-1]

            # Extract model name (everything before the backend)
            model_name = "_".join(file_name.replace(".pte", "").split("_")[:-1])

            # Create model entry
            model_entry = {
                "name": file_name,
                "modelName": model_name,
                "category": category,
                "backend": backend,
                "hash": file_hash,
                "size": file_size,
                "sizeMB": round(file_size / (1024 * 1024), 2),
                "inputSize": config["inputSize"],
                "remoteUrl": f"https://raw.githubusercontent.com/abdelaziz-mahdy/executorch_flutter_models/main/{category}/{file_name}",
            }

            # Add labels reference for the model
            if config["labelsFile"]:
                model_entry["labelsFile"] = config["labelsFile"]
                model_entry["labelsRemoteUrl"] = config["labelsUrl"]

            # Add backend info if available
            if backend in backend_info:
                model_entry["platforms"] = backend_info[backend]["platforms"]
                model_entry["backendDescription"] = backend_info[backend]["description"]

            models.append(model_entry)
            print(f"  âœ“ {file_name} ({model_entry['sizeMB']} MB)")

    # Create labels entries
    labels = []
    for category, config in model_configs.items():
        if config["labelsFile"]:
            labels_path = output_path / category / config["labelsFile"]
            if labels_path.exists():
                # Calculate hash for labels file
                sha256_hash = hashlib.sha256()
                with open(labels_path, "rb") as f:
                    sha256_hash.update(f.read())

                labels.append({
                    "name": config["labelsFile"],
                    "category": category,
                    "hash": sha256_hash.hexdigest(),
                    "size": labels_path.stat().st_size,
                    "remoteUrl": config["labelsUrl"],
                })

    # Create the index structure
    index = {
        "version": "1.0",
        "generated": __import__("datetime").datetime.utcnow().isoformat() + "Z",
        "baseUrl": "https://raw.githubusercontent.com/abdelaziz-mahdy/executorch_flutter_models/main",
        "models": models,
        "labels": labels,
        "backends": backend_info,
    }

    # Write index.json
    index_file = output_path / "index.json"
    with open(index_file, "w") as f:
        json.dump(index, f, indent=2)

    print(f"\nâœ… Generated {index_file}")
    print(f"   Models: {len(models)}")
    print(f"   Labels: {len(labels)}")

    return True


def cmd_export(args):
    """Export command."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                  â•‘
â•‘        ExecuTorch Flutter - Model Export                         â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # Display backend information
    backends = args.backends if hasattr(args, 'backends') and args.backends else None
    if backends:
        print(f"\nğŸ“¦ Exporting with backends: {', '.join(backends)}")
    else:
        print(f"\nğŸ“¦ Exporting with default backends: xnnpack, coreml, mps, vulkan")

    success_count = 0
    total_count = 0

    # Determine what to export
    export_mobilenet_flag = args.all or args.mobilenet
    export_yolo_models = []
    # Note: Gemma is NOT included in --all because it requires HuggingFace authentication
    # Users must explicitly use --gemma flag
    export_gemma_flag = args.gemma

    if args.all:
        export_yolo_models = ["yolo11n", "yolov8n", "yolov5n"]  # All nano YOLO models
    if args.yolo:
        export_yolo_models.extend(args.yolo)

    # Export MobileNet
    if export_mobilenet_flag:
        total_count += 1
        if export_mobilenet(args.output_dir, backends):
            success_count += 1

    # Export YOLO models
    for model_name in export_yolo_models:
        total_count += 1
        if export_yolo(model_name, args.output_dir, backends):
            success_count += 1

    # Export Gemma
    if export_gemma_flag:
        total_count += 1
        if export_gemma("gemma-3-270m", args.output_dir):
            success_count += 1

    # Export labels
    if args.all or args.labels:
        export_labels(args.output_dir)

    # Always generate index.json after export
    generate_index_json(args.output_dir)

    # Summary
    print("\n" + "="*70)
    print("  Export Summary")
    print("="*70)
    print(f"\nâœ… Successfully exported: {success_count}/{total_count} models")

    if success_count < total_count:
        print(f"âŒ Failed exports: {total_count - success_count}")

    print(f"\nğŸ“ Models saved to: {args.output_dir}")
    print(f"\nğŸš€ Next: cd example && flutter run\n")

    return 0 if success_count == total_count else 1


def cmd_validate(args):
    """Validate command."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                  â•‘
â•‘        ExecuTorch Flutter - Model Validation                     â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # Create validator
    validator = ModelValidator(
        models_dir=args.models_dir,
        images_dir=args.images_dir,
        assets_dir="../example/assets"
    )

    # Run validation
    results = validator.validate_all()

    if not results:
        print("\nâŒ Validation failed - no results generated")
        return 1

    # Save results
    output_path = Path(args.output_file)
    output_path.write_text(__import__('json').dumps(results, indent=2))

    # Print summary
    print(f"\n{'=' * 70}")
    print("  Validation Summary")
    print(f"{'=' * 70}\n")

    summary = results['summary']
    print(f"âœ… Total Models: {summary['total_models_tested']}")
    print(f"   - Classification: {summary['classification_models_count']}")
    print(f"   - Detection: {summary['detection_models_count']}")
    print(f"\nâœ… Successful: {summary['successful_models']}")
    print(f"âŒ Failed: {summary['failed_models']}")
    print(f"\nğŸ“¸ Test Images: {summary['total_test_images']}")
    print(f"\nğŸ“„ Results: {output_path}\n")

    return 0 if summary['failed_models'] == 0 else 1


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="ExecuTorch Flutter - Model Export & Validation Tool",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py                                    # Export all models with all backends (default)
  python main.py export --mobilenet                 # Export MobileNet with all backends
  python main.py export --mobilenet --backends xnnpack coreml  # MobileNet with specific backends
  python main.py export --yolo yolo11n              # Export YOLO11n with all backends
  python main.py export --all --backends xnnpack    # Export all models with XNNPACK only
  python main.py export --gemma                     # Export Gemma text generation model
  python main.py validate                           # Validate all models

Supported YOLO models:
  yolo11n, yolo11s, yolov8n, yolov8s, yolov5n, yolov5s

Supported Text Generation models:
  gemma-3-270m (Google Gemma 3, 270M parameters, text-only, ~240 MB)
  gemma-3-1b   (Google Gemma 3, 1B parameters, text-only, ~1.5 GB)

Backend Information:
  xnnpack  - CPU-optimized, works on ALL platforms (Android, iOS, macOS, Web)
  coreml   - Apple Neural Engine optimization (iOS, macOS only)
  mps      - Metal Performance Shaders GPU acceleration (iOS, macOS only)
  vulkan   - Cross-platform GPU acceleration (Android, Linux)
  qnn      - Qualcomm AI Engine (Snapdragon devices)
  arm      - ARM Ethos-U NPU (embedded devices)

Default backends: xnnpack, coreml, mps, vulkan
Note: Only available backends on your system will be used.

Performance Guidelines:
  - iOS/macOS: Use CoreML for best performance (Apple Neural Engine)
  - iOS/macOS GPU: Use MPS for GPU-accelerated inference
  - Android GPU: Use Vulkan for GPU-accelerated inference
  - Qualcomm devices: Use QNN for NPU acceleration
  - All platforms (including Web): XNNPACK works everywhere with good performance

Note: Gemma models require additional setup (advanced):
  1. Install optimum-executorch from source:
     git clone https://github.com/huggingface/optimum-executorch.git
     cd optimum-executorch && pip install '.[dev]' && python install_dev.py
  2. Request access: https://huggingface.co/google/gemma-3-270m-it
  3. Login: hf auth login
  4. Export: python main.py export --gemma
        """
    )

    subparsers = parser.add_subparsers(dest='command', help='Command to run')

    # Export command
    export_parser = subparsers.add_parser('export', help='Export models')
    export_parser.add_argument('--all', action='store_true', help='Export all models')
    export_parser.add_argument('--mobilenet', action='store_true', help='Export MobileNet')
    export_parser.add_argument('--yolo', nargs='+', metavar='MODEL', help='Export YOLO model(s)')
    export_parser.add_argument('--gemma', action='store_true', help='Export Gemma text generation model')
    export_parser.add_argument('--labels', action='store_true', help='Generate label files')
    export_parser.add_argument('--backends', nargs='+',
                                choices=['xnnpack', 'coreml', 'mps', 'vulkan', 'qnn', 'arm'],
                                help='Backend(s) to export for (default: xnnpack, coreml, mps, vulkan)')
    export_parser.add_argument('--output-dir', default='..',
                                help='Output directory (default: .. - parent models/ directory)')

    # Validate command
    validate_parser = subparsers.add_parser('validate', help='Validate models')
    validate_parser.add_argument('--models-dir', default='../assets/models',
                                  help='Models directory (default: ../assets/models)')
    validate_parser.add_argument('--images-dir', default='../assets/images',
                                  help='Test images directory (default: ../assets/images)')
    validate_parser.add_argument('--output-file', default='../assets/model_test_results.json',
                                  help='Output file (default: ../assets/model_test_results.json)')

    args = parser.parse_args()

    # Default to export if no command specified
    if args.command is None:
        args.command = 'export'
        args.all = True
        args.mobilenet = False
        args.yolo = None
        args.gemma = False
        args.labels = True
        args.output_dir = '..'

    # Run command
    if args.command == 'export':
        return cmd_export(args)
    elif args.command == 'validate':
        return cmd_validate(args)
    else:
        parser.print_help()
        return 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
